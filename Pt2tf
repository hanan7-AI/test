Hereâ€™s how to create, train, save, and convert a simple model in PyTorch to TensorFlow Lite:

1. Create and Train a Simple PyTorch Model

This example uses a basic feedforward neural network.

import torch
import torch.nn as nn
import torch.optim as optim

# Define a simple PyTorch model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(50, 2)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Initialize model, loss, and optimizer
model = SimpleModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Dummy training data
x_train = torch.randn(100, 10)
y_train = torch.randint(0, 2, (100,))

# Train the model
for epoch in range(5):  # 5 epochs
    optimizer.zero_grad()
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch + 1}, Loss: {loss.item()}")

# Save the trained model
torch.save(model.state_dict(), "simple_model.pth")

2. Export the PyTorch Model to ONNX

Export the trained model to ONNX format.

dummy_input = torch.randn(1, 10)  # Input shape: [batch_size, features]
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
    opset_version=11
)
print("Model exported to ONNX format.")

3. Convert ONNX to TensorFlow

Use onnx-tf to convert the ONNX model to TensorFlow.

	1.	Install onnx-tf if not already installed:

pip install onnx-tf


	2.	Convert ONNX to TensorFlow:

onnx-tf convert -i simple_model.onnx -o simple_model.pb



4. Convert TensorFlow Model to TensorFlow Lite

Convert the TensorFlow model to TFLite format.

import tensorflow as tf

# Convert TensorFlow model to TFLite
converter = tf.lite.TFLiteConverter.from_saved_model("simple_model.pb")
tflite_model = converter.convert()

# Save the TFLite model
with open("simple_model.tflite", "wb") as f:
    f.write(tflite_model)

print("Model converted to TFLite.")

5. Test the TFLite Model

Verify the TFLite model works as expected.

import numpy as np

# Load the TFLite model
interpreter = tf.lite.Interpreter(model_path="simple_model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Create dummy input data
input_data = np.random.random_sample(input_details[0]['shape']).astype(np.float32)

# Run inference
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()

# Get output
output_data = interpreter.get_tensor(output_details[0]['index'])
print("Output from TFLite model:", output_data)

This process ensures that you:

	1.	Create and train a simple PyTorch model.
	2.	Convert it to TFLite.
	3.	Verify the conversion works as expected.

Let me know if you have questions or need help debugging!
